services:
  ollama:
    image: ollama/ollama
    container_name: ollama
    restart: unless-stopped
    volumes:
      - ../../lib/ollama:/root/.ollama
#    ports:
#      - 11434:11434
    networks:
      - llama
    env_file:
      - .env
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: "${GPU_COUNT}"
              capabilities: [gpu]
networks:
  llama:
    name: llama-net
